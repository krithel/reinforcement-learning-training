{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference Learning\n",
    "\n",
    "Chapter 6 follows the use of Temporal-Difference (TD) learning - a method which combines the approaches of the Dynamic Programming and Monte Carlo approaches to learning. With TD learning - we update our understanding of the values as we go along, rather than at the end of an episode (as we did with Monte Carlo approaches). We are still learning through independant trials, so we don't need to fully understand the environment as we do using DP approaches, but we learn as we go through the episode.\n",
    "\n",
    "This method in practice converges faster than Monte Carlo (although there is no formal proof that this is always the case), and is also suitable for a wider range of problems, as we no longer need to wait until the end of an episode to learn. We will demonstrate the TD approach to value prediction and the control problem using the Windy Gridworld example.\n",
    "\n",
    "## Windy Gridworld\n",
    "\n",
    "Windy Gridworld is an environment our agent can act in. The world is an NxM grid, with a start & goal position for the player, and the player can move in a cardinal direction. Windy gridword adds wind in, which applies to some columns and pushes the player up a square. This provides a simple environment to implement the basic concepts of TD learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "class Position:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Position(self.x + other.x, self.y + other.y)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(repr(self))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '({0}, {1})'.format(self.x, self.y)\n",
    "\n",
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3 \n",
    "    \n",
    "    def position(self):\n",
    "        if self == Action.UP:\n",
    "            return Position(0, -1)\n",
    "        elif self == Action.DOWN:\n",
    "            return Position(0, 1)\n",
    "        elif self == Action.LEFT:\n",
    "            return Position(-1, 0)\n",
    "        elif self == Action.RIGHT:\n",
    "            return Position(1, 0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "class WindyGridworld:\n",
    "    \n",
    "    def __init__(self, width: int, height: int, start: Position, goal: Position, windy_cols):\n",
    "        if len(windy_cols) != width:\n",
    "            raise Exception('Number of windy cols must match the width of the grid')\n",
    "        \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.windy_cols = windy_cols\n",
    "        \n",
    "        self.current_position = start\n",
    "        \n",
    "    def act(self, action: Action):\n",
    "        wind = self.windy_cols[self.current_position.x]\n",
    "        self.current_position += action.position()\n",
    "        \n",
    "        # Apply the wind\n",
    "        self.current_position.y -= wind\n",
    "        \n",
    "        # Bound the player to the walls of the Grid\n",
    "        if (self.current_position.x < 0):\n",
    "            self.current_position.x = 0\n",
    "        elif (self.current_position.x >= self.width):\n",
    "            self.current_position.x = self.width - 1\n",
    "        \n",
    "        if (self.current_position.y < 0):\n",
    "            self.current_position.y = 0\n",
    "        elif (self.current_position.y >= self.height):\n",
    "            self.current_position.y = self.height - 1\n",
    "            \n",
    "        return self.current_position\n",
    "    \n",
    "    def reward(self):\n",
    "        if self.current_position == self.goal:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def terminal(self):\n",
    "        return self.current_position == self.goal\n",
    "    \n",
    "    def __repr__(self):\n",
    "        world = np.zeros((self.height, self.width))\n",
    "        world[self.start.y, self.start.x] = 1\n",
    "        world[self.goal.y, self.goal.x] = 2\n",
    "        world[self.current_position.y, self.current_position.x] = 3\n",
    "        \n",
    "        return str(world) + '\\n\\n ' + str(self.windy_cols)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa\n",
    "\n",
    "The Sarsa algorithm uses TD(0) to determine the value, and then a policy (in this case we are using epsilon greedy) to determine the action to take based off the value estime. TD(0) updates the value estimates mid-episode (as opposed to Monte Carlo which does it at the end of the episode) so it learns every action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our policy. We will use an epsilon-greedy policy\n",
    "def epsilon_greedy(state, q_values, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(list(Action))\n",
    "    \n",
    "    \n",
    "    current_max = None\n",
    "    current_actions = []\n",
    "\n",
    "    for a in Action:\n",
    "        val = q_values[(state, a)]\n",
    "        if current_max == None or val > current_max:\n",
    "            current_max = val\n",
    "            current_actions = [a]\n",
    "        elif val == current_max:\n",
    "            current_actions.append(a)\n",
    "    \n",
    "    return random.choice(current_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 episodes complete in 24320 steps\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "HEIGHT = 7\n",
    "WIDTH = 10\n",
    "\n",
    "Q = {(Position(x, y), action): 0 for action in Action for x in range(0, WIDTH) for y in range(0, HEIGHT)}\n",
    "START = Position(0, 3)\n",
    "GOAL = Position(7, 3)\n",
    "WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
    "\n",
    "EPISODES = 1000\n",
    "ALPHA = 0.5\n",
    "EPSILON = 0.1\n",
    "GAMMA = 1\n",
    "\n",
    "policy = epsilon_greedy\n",
    "\n",
    "timesteps = 0\n",
    "\n",
    "for e in range(0, EPISODES):\n",
    "    environment = WindyGridworld(WIDTH, HEIGHT, START, GOAL, WIND)\n",
    "    s = START\n",
    "    a = policy(s, Q, EPSILON)\n",
    "\n",
    "    while not environment.terminal():\n",
    "        s_prime = environment.act(a)\n",
    "        r = environment.reward()\n",
    "        a_prime = policy(s_prime, Q, EPSILON)\n",
    "        Q[(s, a)] = Q[(s, a)] + ALPHA * (r + GAMMA * Q[(s_prime, a_prime)] - Q[(s, a)])\n",
    "        \n",
    "        s = s_prime\n",
    "        a = a_prime\n",
    "        \n",
    "        timesteps += 1\n",
    "        \n",
    "print(\"{} episodes complete in {} steps\".format(EPISODES, timesteps))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average steps per episode: 15.0\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_STEPS = 1\n",
    "total_steps = 0\n",
    "\n",
    "for i in range(0, VALIDATION_STEPS):\n",
    "    validation_env = WindyGridworld(WIDTH, HEIGHT, START, GOAL, WIND)\n",
    "    s = START\n",
    "    actions = []\n",
    "    states = [s]\n",
    "    while not validation_env.terminal():\n",
    "        a = policy(s, Q, 0)\n",
    "        actions.append(a)\n",
    "        s = validation_env.act(a)\n",
    "        states.append(s)\n",
    "        \n",
    "    total_steps += len(actions)\n",
    "    \n",
    "print(\"Average steps per episode: {}\".format(total_steps / VALIDATION_STEPS))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_to_letter(action):\n",
    "    if action == Action.LEFT:\n",
    "        return 'L'\n",
    "    elif action == Action.RIGHT:\n",
    "        return 'R'\n",
    "    elif action == Action.UP:\n",
    "        return 'U'\n",
    "    elif action == Action.DOWN:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return '?'\n",
    "    \n",
    "t = [[action_to_letter(epsilon_greedy(Position(x, y), Q, 0)) for x in range(0, WIDTH)] for y in range(0, HEIGHT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [['X' if Position(x, y) in states else ' ' for x in range(0, WIDTH)] for y in range(0, HEIGHT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' ', ' ', ' ', ' ', ' ', ' ', 'X', 'X', 'X', 'X'],\n",
       " [' ', ' ', ' ', ' ', ' ', 'X', ' ', ' ', ' ', 'X'],\n",
       " [' ', ' ', ' ', ' ', 'X', ' ', ' ', ' ', ' ', 'X'],\n",
       " ['X', 'X', 'X', 'X', ' ', ' ', ' ', 'X', ' ', 'X'],\n",
       " [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'X', 'X'],\n",
       " [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],\n",
       " [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
